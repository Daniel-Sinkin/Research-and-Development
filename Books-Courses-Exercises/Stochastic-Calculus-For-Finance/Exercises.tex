\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\mse}{\operatorname{mse}}
\newcommand{\bias}{\operatorname{bias}}
\newcommand{\se}{\operatorname{se}}
\newcommand{\Bernoulli}{\operatorname{Bernoulli}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\Poisson}{\operatorname{Poisson}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\MLE}{\operatorname{MLE}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\F}{\mathcal{F}}


\usepackage{enumitem}

\usepackage{graphicx}

\usepackage{subcaption}

\usepackage{minted}

\usepackage{hyperref}

\begin{document}

\tableofcontents

\newpage

\section{General Probability Theory}
\begin{enumerate}
\item Using the properties of Definition 1.1.2 for a probability measure $P$, show the following.
\begin{enumerate}
	\item If $A \in \F, B \in \F$, and $A \subseteq B$ then $P(A) \leq P(B)$.
		\begin{itemize}
			\item Note that $A \cap B \backslash A = \emptyset$ as such
			$$
			P(B) = P(A) + P(B \backslash A) \geq P(A).
			$$
		\end{itemize}
	\item If $A \in \F$ and $\{A_n\}_{n = 1}^\infty$ is a sequence of sets in $\F$ with
	$$
	\lim_{n \rightarrow \infty} P(A_n) = 0
	$$
	and $A \subseteq A_n$ for every $n$, then $P(A) = 0$. (This property was used implicitly in Example 1.1.4 when we argued that the sequence of all heads, and indeed any particular sequence, mus have probability zero.)
		\begin{itemize}
			\item This is just the sandwitch lemma:
			$$
			0 \leq P(A) \leq P(A_n) \rightarrow 0
			$$
			implies $P(A) = 0$.
		\end{itemize}
\end{enumerate}
	\item The infinite coin-toss space $\Omega_\infty$ of example 1.1.4 is uncountable infinite. Suppose that were was a sequential list
	$$
	\omega^{(i)} = \omega^{(i)}_1 \omega^{(i)}_2 \omega^{(i)}_3 \dots
	$$
	where $i = 1, 2, \dots$ of all elements of $\Omega_\infty$, i.e. such that
	$$
	\bigcup_i \omega^{(i)} = \Omega_\infty.
	$$
	An element that does not appear in this list is the sequence whose first component is $H$ if $\omega_1^{(1)}$ is $T$ and is $T$ if $\omega_1^{(1)}$ is $H$, and so on. As such there is no such sequence $(\omega^{(i)})$.

	Now consider the set $A \subseteq \Omega_\infty$ such that for all $\omega \in A$ the elements $\omega_{2k - 1}$ and $\omega_{2k}$ are the same for all $k \geq 1$.
		\begin{enumerate}
			\item Show that $A$ is uncountably infinite.
				\begin{itemize}
					\item Suppose there is a counting $\alpha^{(i)} = \alpha_1^{(i)}\alpha_2^{(i)}\alpha_3^{(i)}\dots$ then
					$$
					\omega_j^{(i)} := \alpha_{2j - 1}^{(i)}
					$$
					is a counting of $\Omega_\infty$, which is impossible.
				\end{itemize}
			\item Show that, when $0 < p < 1$, we have $P(A) = 0$.
				\begin{itemize}
					\item Let $A_{k}$ be defined such that $\omega A_k$ means $\omega_1 = \omega_2, \dots, \omega_{2k - 1} = \omega_{2k}$ and note that $A \subseteq A_k$ for all $k$. Furthermore it can inductively be shown that
					$$
					P(A_k) = (p^2 + (1 - p)^2)^k
					$$
					which converges to $0$ whenever $p \in (0, 1)$. $P(A)$ then follows by Exercise 1.1(ii).
				\end{itemize}
		\end{enumerate}
	\item Consider the set function $P$ defined for every subset of $[0, 1]$ by $P(A) = 0$ if $|A| < \infty$ and $P(A) = \infty$ if $|A| = \infty$. Show that $P$ satisfies (1.1.3)-(1.1.5), but $P$ does not have the countable additivity property (1.1.2). We see then that the finite additivity property (1.1.5) does not imply the countable additivity property (1.1.2).
		\begin{itemize}
			\item[1.1.3] $|\emptyset| < 0$ so $P(\emptyset) = 0$. 
			\item[1.1.4] If $A$ and $B$ are both finite then the union is also finite. If at least one of them is not finite then the union has infinite $P$ value. In either case the identity holds. 
			\item[1.1.5] Follows inductively from 1.1.4.
			\item[-1.1.2] Consider the sequence $x_n := 1 / n$ then $P(\{x_n\} = 0$ for all $n$ but $P\left(\bigcup \{x_n\}\right) = +\infty$.
		\end{itemize}
	\item
		\begin{enumerate}
			\item Construct a standard normal random variable $Z$ on the probability space $(\Omega_\infty, \F_\infty, P)$ of example 1.1.4 under the assumption that the probability for head is $p = 1 / 2$. (Hint: Consider Examples 1.2.5 and 1.2.6).
			\item Define a sequence of random variables $\{Z_n\}_{n = 1}^\infty$ on $\Omega_\infty$ such that $Z_n \rightarrow Z$ pointwise, and, for each $n$, $Z_n$ only depends on the first $n$ coint tosses. (This gives us a procedure for approximating a standard normal random variable by random variables generated by a finite number of coin tosses, a useful algorithm for Monte Carlo simulation.)
		\end{enumerate}
	\item When dealing with double Lebesgue integrals, just as with doubel Riemann integrals, the order of integration can be reversed. The only assumption required is that the function being integrated be either nonnegative or integrable. Here is an application of this fact.

	Let $X$ be a nonnegative random variable with cumulative distribution function $F(x) = P(X \leq x)$. Show that
	$$
	EX = \int_0^\infty (1 - F(x)) dx
	$$
	by showing that
	$$
	\int_\Omega \int_0^\infty 1_{[0, X(\omega))}(x) dx dP(\omega)
	$$
	is equal to both $EX$ and
	$$
	\int_0^\infty (1 - F(x))dx.
 	$$
 		\begin{itemize}
 			\item Consider the following computation,
 			$$
 			\begin{aligned}
 			EX &= \int_\Omega X(\omega) dP(\omega) \\
 			&= \int_\Omega \lambda^1([0, X(\omega))) dP(\omega) \\
 			&= \int_\Omega \int_0^\infty 1_{[0, X(\omega))}(x) dx dP(\omega) \\
 			&\overset{*}{=} \int_0^\infty \int_\Omega 1_{[0, X(\omega))}(x) dP(\omega) \\
 			&= \int_0^\infty P(X < x) dx \\
 			&= \int_0^\infty 1 - F(x) dx,
 			\end{aligned}
 			$$
 			where we have used Fubini's theorem inf $*$.
		\end{itemize}
 	\item Let $u$ be a fixed number in $\mathbb{R}$, and define the convex function
 	$$
 	\varphi(x) = e^{ux}
 	$$
 	for all $x \in \mathbb{R}$. Let $X$ be a normal random variable with mean $\mu = EX$ and standard deviation
 	$$
 	\sigma = (E(X - \mu))^{1 / 2}.
 	$$
 	\begin{enumerate}
 		\item Verify that
 		$$
 		Ee^{uX} = e^{u\mu + \frac{1}{2} u^2 \sigma^2}.
 		$$
 		\item Verify that Jensen's inequality holds (as it must):
 			$$
 			E\varphi(X) \geq \varphi(EX).
 			$$
	\end{enumerate}
	\item For each positive integer $n$, define $f_n$ to be normal density with mean zero and variance $n$, i.e.,
	$$
	f_n(x) = \frac{1}{\sqrt{2n\pi}}e^{- x^2 / 2n}.
	$$
	\begin{enumerate}
	\item Determine the pointwise limit
	$$
	f(x) = \lim_{n \rightarrow \infty} f_n(x).
	$$
		\begin{itemize}
			\item $f_n \rightarrow 0$ pointwise.
		\end{itemize}
	\item Calculate
	$$
	\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f_n(x) dx.
	$$
		\begin{itemize}
			\item Note that $f_n$ is a PDF for every $n$, as such the integrals are all equal to $1$, as such
			$$
			\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f_n(x) dx = +1.
			$$
		\end{itemize}
	\item Note that
	$$
	\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f_n(x) dx \neq \int_{-\infty}^\infty f(x) dx.
	$$
	Explain why this does not violate the Monotone Convergence Theorem, Theorem 1.4.5.
		\begin{itemize}
			\item $f_n$ is not a monotone sequence.
		\end{itemize}
	\end{enumerate}
	\item Let $X$ be a nonnegative random variable, and assume that
	$$
	\varphi(t) = Ee^{tX}
	$$
	is finite for every $t \in \R$. Assume further that $E\left[Xe^{tx}\right] < \infty$ for every $t \in \R$. The purpose of this exercise is to show that $\varphi'(t) = E\left[Xe^{tX}\right]$ and, in particular, $\varphi'(0) = EX$.

	The Mean Value Theorem for our case can be stated as follows: Let $\omega \in \Omega$ be fixed and define $f(t) = e^{tX(\omega))}$, then this becomes
	$$
	e^{tX(\omega)} - e^{sX(\omega)} = (t - s)X(\omega)e^{\theta(\omega)X(\omega)},
	$$
	where $\theta(\omega)$ is a number depending on $\omega$ (i.e., a random variable lying between $t$ and $s$.
		\begin{enumerate}
			\item Use the Dominated Convergence Theorem (Theorem 1.4.9) and equation (1.9.1) to show that
			$$
			\lim_{n \rightarrow \infty} EY_n = E\left[\lim_{n \rightarrow \infty} Y_n\right] = E\left[Xe^{tX}\right].
			$$
			This establishes the desired formula
			$$
			\varphi'(t) = E\left[Xe^{tX}\right].
			$$
			\item Suppose the random variable $X$ can take both positive and negative values and $Ee^{tX} < \infty$ and $E[|X|e^{tX}] < \infty$ for every $t \in \R$. Show that once again
			$$
			\varphi'(t) = E\left[Xe^{tX}\right].
			$$
			Hint: Use the notation (1.3.1) to write $X = X^+ - X^-$.
		\end{enumerate}
	\item Suppose $X$ is a random variable on some probability space $(\Omega, \mathcal{F}, \mathbb{P})$, $A$ is a set in $\mathcal{F}$, and for evvery Borel subset $B$ of $\R$, we have
	$$
	\int_A 1_B(X(\omega))dP(\omega) = P(A)P(X \in B).
	$$
	Then we say that $X$ is independent of the event $A$. Show that if $X$ is independent of an event $A$, then
	$$
	\int_A g(X(\omega)) dP(\omega) = P(A) Eg(X)
	$$
	for every nonnegative, Borel-measureable function $g$.
	\item 
	\item 
	\item 
	\item 
	\item 
	\item 
\end{enumerate}

\section{Information and Conditioning}
\begin{enumerate}
	\item Let $(\Omega, \F, P)$ be a general probability space, and suppose a random variable $X$ on this space is measureable with respect to the trivial $\sigma$-algebra $\mathcal{F}_0 = \{\emptyset, \Omega\}$. Show that $X$ is not random (i.e., there is a constant $c$ such that $X(\omega) = c$ for all $\omega \in \Omega$). Such a random variable is called degenerate.
	\item Independence of random variables can be affected by changes of measure. To illustrate this point, consider the space of two coin tosses $\Omega_2 = \{HH, HT, TH, TT\}$, and let stock prices be given  by
	$$
	\begin{aligned}
	S_0 = 4, S_1(H) = 8, S_1(T) = 2, \\
	S_2(HH) = 16, S_2(HT) = S_2(TH) = 4, S_2(TT) = 1.
	\end{aligned}
	$$
	Consider two probability measures given by
	$$
	\begin{aligned}
	\tilde{P}(HH) = 1/4, \tilde{P}(HT) = 1/4, \tilde{P}(TH) = 1/4, \tilde{P}(HH) = 1/4, \\
	P(HH) = 4/9, P(HT) = 2/9, P(TH) = 2/9, P(TT) = 1/9.
	\end{aligned}
	$$
	Define the random variable
	$$
	X =
	\begin{cases}
		1,&\text{if }S_2 = 4,\\
		0,&\text{if }S_2 \neq 4
	\end{cases}.
	$$
		\begin{enumerate}
			\item List all the sets in $\sigma(X)$.
				\begin{itemize}
					\item Note that $\{X = 1\} = \{HT, TH\}$ and $\{X = 0\} = \{X = 1\}^c = \{HH, TT\}$. As such
					$$
					\sigma(X) = \{\emptyset, \{HT, TH\}, {HH, TT}, \Omega_2\}.
					$$
				\end{itemize}
			\item List all the sets in $\sigma(S_1)$.
				\begin{itemize}
					\item Note that $\{S_1 = 8\} = \{HH, HT\}$ and $\{S_2 = 2\} = \{TH, TT\}$. As such
					$$
					\sigma(S_1) = \{\emptyset, \{HH, HT\}, {TH, TT}, \Omega_2\}.
					$$
				\end{itemize}
			\item Show that $\sigma(X)$ and $\sigma(S_1)$ are independent under the probability measure $\tilde{P}$.
				\begin{itemize}
					\item
					$$
					\begin{aligned}
					\tilde{P}(\{HT, TH\} \cap \{HH, HT\}) &= \tilde{P}(\{HT\}) = 1/4 \\
					\tilde{P}(\{HT, TH\} \cap \{TH, TT\}) &= \tilde{P}(\{TH\}) = 1/4 \\
					\tilde{P}(\{HH, TT\} \cap \{HH, HT\}) &= \tilde{P}(\{HH\}) = 1/4 \\
					\tilde{P}(\{HH, TT\} \cap \{TH, TT\}) &= \tilde{P}(\{TT\}) = 1/4 \\
					\end{aligned}
					$$
				\end{itemize}
				Because $\tilde{P}$ assigns sets of size $2$ a probability of $1/4$ the corresponding products are always $1/4$, as such $\sigma(X)$ and $\sigma(S_1)$ are independent with respect to $\tilde{P}$.
			\item Show that $\sigma(X)$ and $\sigma(S_1)$ are not independent under the probability measure $P$.
				\begin{itemize}
					\item Note that $P(\{HT, TH\}) = P(HT) + P(TH) = 2/9 + 2/9$ and $P(\{HH, HT\}) = 4/9 + 2/9 = 6/9$. As such the product of those is equal to $32 / 81$.
					\item On the other hand the probability of the intersection is given by
					$$
					P(HT) = 2/9.
					$$
				\end{itemize}
			\item Under $P$, we have $P(S_1 = 8) = 2/3$ and $P(S_1 = 2) = 1/3$. Explain intuitively why, if you are told that $X = 1$, you would want to revise your estimate of the distribution of $S_1$.
		\end{enumerate}
	\item Let $X$ and $Y$ be independent standard random variables. Let $\theta$ be a constant, and define random variables
	$$
	\begin{aligned}
	V = X \cos \theta + Y \sin \theta,&& W = -X \sin \theta + Y \cos \theta.
	\end{aligned}
	$$
	Show that $V$ and $W$ are independent standard normal random variables.
	\item 
	\item Let $(X, Y)$ be a pair of random variables with joint density function
	$$
	f_{X, Y}(x, y) = \frac{2 |x| + y}{\sqrt{2\pi}}\exp\left( - \frac{(2|x| + y)^2}{2} \right)
	$$
	for $y \geq -|x|$ and $f_{X, Y}(x, y) = 0$ otherwise. How that $X$ and $Y$ are standard normal random variables and that they are uncorrelated but not independent.
	\item 
	\item
	\item Let $X$ and $Y$ be integrable random variables on a probability space $(\Omega, \F, P)$. Then $Y = Y_1 + Y_2$, where $Y_1 = E[Y|X]$ os $\sigma(X)$-measurable and $Y_2 = Y - E[Y|X]$. Show that $Y_2$ and $X$ are uncorrelated. More generally, show that $Y_2$ is uncorrelated with every $\sigma(X)$-measureable random variable.
	\item
	\item
	\item
		\begin{enumerate}
			\item Let $X$ be a random variable on a probability space $(\Omega, \F, P)$, and let $W$ be a nonegative $\sigma(X)$-measureable random variable. Show there exists a function $g$ such that $W = g(X)$. (Hint: Recall that every set in $\sigma(X)$ is of the form $\{X \in B\}$ for some Borel set $B \subseteq \mathbb{R}$. Suppose first that $W$ is the indicator of such a set, and then use the standard machine.)
			\item Lett $X$ be a random variable on a probability space $(\Omega, \F, P)$, and let $Y$ be a nonnegative random variable on this space. We do not assume that $X$ and $Y$ have a joint density. Nonetheless, show there is a function $g$ such that $E[Y|X] = g(X)$.
		\end{enumerate}
\end{enumerate}

\section{Brownian Motion}
\begin{enumerate}
	\item According to Definition 3.3.3(iii), for $0 \leq t < u$, the Brownian motion increment $W(u) - W(t)$ is independent of the $\sigma$-algebra $\mathcal{F}(t)$. Use this property and property (i) of that definition to show that, for $0 \leq t < u_1 < u_2$, the increment $W(u_2) - W(u_1)$ is also independent of $\mathcal{F}(t)$.
	\item Let $W(t), t \geq 0$ be a Brownian motion, and let $\mathcal{F}(t), t \geq 0$, be a filtration for this Brownian motion. Show that $W^2(t) - t$ is a martingale.

	Hint: For $0 \leq s \leq t$, write $W^2(t)$ as $(W(t) - W(s))^2 + 2W(t)W(s) - W^2(s)$.
\end{enumerate}

\section{Stochastic Calculus}
\section{Risk-Neutral Pricing}
\section{Connections with Partial Differential Equations}
\section{Exotic Options}
\section{American Derivative Securities}
\section{Change of NumÃ©raire}
\section{Term-Structure Models}
\section{Introduction to Jump Processes}

\end{document}
